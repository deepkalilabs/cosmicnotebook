[{"id": "435eed9e-751b-4a86-8173-8f49951bb97f", "code": "\"\"\" \nTODO:\n- Support file upload as a \"cell\"\n- Support env variables \n- make the toolbar always float on top\n- turn the newly added cell to focus\n- auto pip install\n- show loading button while code executes\n- autocomplete is dogshit\n\nErrors URGENT:\n- Magic commands are breaking (!pip, !which). hacked something to make them run when they are in a separate cell on the top line. won't work otherwise.\n- magic commands need to be deleted otherwise build will break.\n- caching won't work AT ALL. lambda doesn't allow file writes to the system.\n- reordering cells breaks the cells.\n\"\"\"", "output": "' \\nTODO:\\n- Support file upload as a \"cell\"\\n- Support env variables \\n- make the toolbar always float on top\\n- turn the newly added cell to focus\\n- auto pip install\\n- show loading button while code executes\\n- autocomplete is dogshit\\n\\nErrors URGENT:\\n- Magic commands are breaking (!pip, !which). hacked something to make them run when they are in a separate cell on the top line. won\\'t work otherwise.\\n- magic commands need to be deleted otherwise build will break.\\n- caching won\\'t work AT ALL. lambda doesn\\'t allow file writes to the system.\\n- reordering cells breaks the cells.\\n'# Execution finished\n", "executionCount": 6}, {"id": "8fd13288-1664-4df9-a53f-77d945a84b84", "code": "import json\nimport warnings\nimport pandas as pd\nfrom pprint import pprint\nimport dspy\nimport os\nfrom pydantic import BaseModel, Field\nfrom dataclasses import dataclass\n\nOPENAI_API_KEY=\"\"\nAWS_REGION=\"\"\nAWS_ACCESS_KEY_ID=\"\"\nAWS_SECRET_ACCESS_KEY=\"\"\nAWS_BUCKET_NAME=\"\"\n\nN = 2.3\nN = 4\nopenai_lm = dspy.LM('openai/gpt-4o-mini', api_key=OPENAI_API_KEY, temperature=0.001*N)\nanthropic_lm = dspy.LM('anthropic/claude-3-5-sonnet-20240620', api_key=os.environ.get('ANTHROPIC_API_KEY'))\ndspy.settings.configure(lm=openai_lm)\nprint(\"dspy.settings.lm\", dspy.settings.lm)\n", "output": "/Users/shikharsakhuja/miniforge3/envs/venv_kernel_1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\ndspy.settings.lm <dspy.clients.lm.LM object at 0x1032c4c70>\n# Execution finished\n", "executionCount": 3}, {"id": "654f87ec-ad09-40af-86b2-b430c2334660", "code": "# import base64\nimport json\n#import logging\nfrom typing import Any, List, Tuple, Union\nimport os\nimport io\n# import numpy as np\nimport pandas as pd\nimport re\n# import matplotlib.pyplot as plt\nimport tiktoken\nfrom diskcache import Cache\nimport hashlib\nimport io\nimport boto3\nfrom urllib.parse import urlparse\n\ndef get_dirs(path: str) -> List[str]:\n    return next(os.walk(path))[1]\n\n\ndef clean_column_name(col_name: str) -> str:\n    \"\"\"\n    Clean a single column name by replacing special characters and spaces with underscores.\n\n    :param col_name: The name of the column to be cleaned.\n    :return: A sanitized string valid as a column name.\n    \"\"\"\n    return re.sub(r'[^0-9a-zA-Z_]', '_', col_name)\n\n\ndef clean_column_names(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Clean all column names in the given DataFrame.\n\n    :param df: The DataFrame with possibly dirty column names.\n    :return: A copy of the DataFrame with clean column names.\n    \"\"\"\n    cleaned_df = df.copy()\n    cleaned_df.columns = [clean_column_name(col) for col in cleaned_df.columns]\n    return cleaned_df\n\n\ndef read_dataframe(file_location: str, encoding: str = 'utf-8') -> pd.DataFrame:\n    \"\"\"\n    Read a dataframe from a given file location and clean its column names.\n    It also samples down to 4500 rows if the data exceeds that limit.\n\n    :param file_location: The path to the file containing the data.\n    :param encoding: Encoding to use for the file reading.\n    :return: A cleaned DataFrame.\n    \"\"\"\n    file_extension = file_location.split('.')[-1]\n    \n    try:\n        if file_location.startswith('s3://'):\n            # Configure S3 client\n            session = boto3.Session(\n                aws_access_key_id=AWS_ACCESS_KEY_ID,\n                aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n                region_name=AWS_REGION\n            )\n            \n            # Parse S3 URI\n            parsed_uri = urlparse(file_location)\n            bucket = parsed_uri.netloc\n            key = parsed_uri.path.lstrip('/')\n            \n            # Use session to read the file\n            s3 = session.client('s3')\n            obj = s3.get_object(Bucket=bucket, Key=key)\n            df = _read_file_by_extension(obj['Body'], file_extension)\n                \n        else:\n            # Handle local files\n            with open(file_location, 'rb') as f:\n                df = _read_file_by_extension(f, file_extension)\n    \n    except Exception as e:\n        raise Exception(f\"Error reading file {file_location}: {str(e)}\")\n    \n    # Clean column names\n    cleaned_df = clean_column_names(df)\n\n    if cleaned_df.columns.tolist() != df.columns.tolist():\n        write_funcs = {\n            'csv': lambda: cleaned_df.to_csv(file_location, index=False, encoding=encoding),\n            'xls': lambda: cleaned_df.to_excel(file_location, index=False),\n            'xlsx': lambda: cleaned_df.to_excel(file_location, index=False),\n            'parquet': lambda: cleaned_df.to_parquet(file_location, index=False),\n            'feather': lambda: cleaned_df.to_feather(file_location, index=False),\n            'json': lambda: cleaned_df.to_json(file_location, orient='records', index=False, default_handler=str),\n            'tsv': lambda: cleaned_df.to_csv(file_location, index=False, sep='\\t', encoding=encoding)\n        }\n\n        if file_extension not in write_funcs:\n            raise ValueError('Unsupported file type')\n\n        try:\n            write_funcs[file_extension]()\n        except Exception as e:\n            print(f\"Failed to write file: {file_location}. Error: {e}\")\n            raise\n\n    return cleaned_df\n\ndef _read_file_by_extension(file_obj, file_extension: str) -> pd.DataFrame:\n    \"\"\"\n    Helper function to read different file formats into a pandas DataFrame\n    \"\"\"\n    if file_extension == 'csv':\n        return pd.read_csv(file_obj)\n    elif file_extension == 'json':\n        return pd.read_json(file_obj, orient='records')\n    elif file_extension in ['xls', 'xlsx']:\n        return pd.read_excel(file_obj)\n    elif file_extension == 'parquet':\n        return pd.read_parquet(file_obj)\n    elif file_extension == 'feather':\n        return pd.read_feather(file_obj)\n    elif file_extension == 'tsv':\n        return pd.read_csv(file_obj, sep=\"\\t\")\n    else:\n        raise ValueError(f'Unsupported file type: {file_extension}')\n\ndef file_to_df(file_location: str):\n    \"\"\" Get summary of data from file location \"\"\"\n    file_name = file_location.split(\"/\")[-1]\n    df = None\n    if \"csv\" in file_name:\n        df = pd.read_csv(file_location)\n    elif \"xlsx\" in file_name:\n        df = pd.read_excel(file_location)\n    elif \"json\" in file_name:\n        df = pd.read_json(file_location, orient=\"records\")\n    elif \"parquet\" in file_name:\n        df = pd.read_parquet(file_location)\n    elif \"feather\" in file_name:\n        df = pd.read_feather(file_location)\n\n    return df", "output": "# Execution finished\n", "executionCount": 30}, {"id": "59d1b166-0ea0-449b-be9f-aafab8eb78ee", "code": "import sys\nimport os\n\nimport json\nimport warnings\nimport pandas as pd\nfrom pprint import pprint\nimport dspy\nimport os\nfrom pydantic import BaseModel, Field\nfrom dataclasses import dataclass\n\nclass DatasetHelper():\n    # TODO: Move this to the models file or a helper folder for models. \n    def __init__(self, csv_file_uri, enriched_columns_properties=None, enriched_dataset_schema=None, save_to_db=False) -> None:\n        self.summary = None\n        self.df = read_dataframe(csv_file_uri)\n        print(\"df\", self.df)\n        self.file_name = csv_file_uri.split(\"/\")[-1]\n        self._column_properties = enriched_columns_properties\n        self._dataset_schema = enriched_dataset_schema\n        self.uri = csv_file_uri\n        \n        \n    def check_type(self, dtype: str, value):\n        \"\"\"Cast value to right type to ensure it is JSON serializable\"\"\"\n        if \"float\" in str(dtype):\n            return float(value)\n        elif \"int\" in str(dtype):\n            return int(value)\n        else:\n            return value\n        \n    @property\n    def enriched_column_properties(self):\n        \"\"\"\n            Detailed properties of each column in the dataset.\n        \"\"\"\n        if self._column_properties is None:\n            self._column_properties = self._calculate_column_properties()\n        return self._column_properties\n\n    def _calculate_column_properties(self, n_samples: int = 3) -> list[dict]:\n        \"\"\"Get properties of each column in a pandas DataFrame\"\"\"\n        self.properties_list = []\n        \n        for column in self.df.columns:\n            dtype = self.df[column].dtype\n            properties = {}\n            if dtype in [int, float, complex]:\n                properties[\"dtype\"] = \"number\"\n                properties[\"std\"] = self.check_type(dtype, self.df[column].std())\n                properties[\"min\"] = self.check_type(dtype, self.df[column].min())\n                properties[\"max\"] = self.check_type(dtype, self.df[column].max())\n\n            elif dtype == bool:\n                properties[\"dtype\"] = \"boolean\"\n            elif dtype == object:\n                # Check if the string column can be cast to a valid datetime\n                try:\n                    with warnings.catch_warnings():\n                        warnings.simplefilter(\"ignore\")\n                        pd.to_datetime(self.df[column], errors='raise')\n                        properties[\"dtype\"] = \"date\"\n                except ValueError:\n                    # Check if the string column has a limited number of values\n                    if self.df[column].nunique() / len(self.df[column]) < 0.5:\n                        properties[\"dtype\"] = \"category\"\n                    else:\n                        properties[\"dtype\"] = \"string\"\n            elif pd.api.types.is_categorical_dtype(self.df[column]):\n                properties[\"dtype\"] = \"category\"\n            elif pd.api.types.is_datetime64_any_dtype(self.df[column]):\n                properties[\"dtype\"] = \"date\"\n            else:\n                properties[\"dtype\"] = str(dtype)\n\n            # add min max if dtype is date\n            if properties[\"dtype\"] == \"date\":\n                try:\n                    properties[\"min\"] = self.df[column].min()\n                    properties[\"max\"] = self.df[column].max()\n                except TypeError:\n                    cast_date_col = pd.to_datetime(self.df[column], errors='coerce')\n                    properties[\"min\"] = cast_date_col.min()\n                    properties[\"max\"] = cast_date_col.max()\n            # Add additional properties to the output dictionary\n            nunique = self.df[column].nunique()\n            if \"samples\" not in properties:\n                non_null_values = self.df[column][self.df[column].notnull()].unique()\n                n_samples = min(n_samples, len(non_null_values))\n                samples = pd.Series(non_null_values).sample(\n                    n_samples, random_state=42).tolist()\n                properties[\"samples\"] = samples\n            properties[\"num_unique_values\"] = nunique\n            # properties[\"semantic_type\"] = \"\"\n            # properties[\"description\"] = \"\"\n            self.properties_list.append(\n                {\"column_name\": column, \"properties\": properties})\n            \n            \n        return self.properties_list\n    \n    @property\n    def enriched_dataset_schema(self):\n        \"\"\"\n            High level schema of the dataset, with description and semantic_type for each column.\n        \"\"\"\n        if self._dataset_schema is None:\n            self._dataset_schema = self._update_dataset_schema(self.enriched_column_properties)\n            \n        return self._dataset_schema\n    \n    def _update_dataset_schema(self, columns):\n        schema_list = []\n        for column_dict in columns:\n            print(column_dict)\n            schema_list.append({\"column_name\": column_dict[\"column_name\"], \"description\": column_dict[\"properties\"][\"description\"], \"semantic_type\": column_dict[\"properties\"][\"semantic_type\"]})\n        \n        return schema_list\n\n    @enriched_dataset_schema.setter\n    def enriched_dataset_schema(self, new_schema):\n        self._dataset_schema = new_schema\n        \n    @enriched_column_properties.setter\n    def enriched_column_properties(self, new_properties_list):\n        print(\"Setting new properties list\")\n        pprint(new_properties_list)\n        self._column_properties = new_properties_list\n        \n        \nclass FieldEnrich(dspy.Signature):\n    \"\"\"\n        Given JSON of field details, generate JSON for the semantic_type and description of the field.\n    \"\"\"\n    print(dspy)\n    field_json = dspy.InputField(desc=\"JSON of field details.\")\n    enriched_field_json = dspy.OutputField(desc=\"JSON for the semantic_type and description of the field. description should be one-liner. semantic_type should be one word with underscore and very descriptive.\")\n    \n#Define a simple signature for basic question answering\nclass EnrichDatasetDescription(dspy.Signature):\n    \"\"\"\n        Update the dataset with dataset_description.\n    \"\"\"\n    schema = dspy.InputField(desc=\"The schema of the dataset\")\n    dataset_description = dspy.OutputField(desc=\"One line description of the dataset\")\n    \nclass DatasetEnrich(dspy.Module):\n    def __init__(self, url: str) -> None:\n        self.dataset = DatasetHelper(url)\n        self.enriched_field_json = dspy.ChainOfThought(FieldEnrich)\n        self.dataset_description = dspy.ChainOfThought(EnrichDatasetDescription)\n        \n    def enrich_fields(self):\n        \"\"\"\n            Enriches each field in the csv with description & semantic_type.\n        \"\"\"\n        column_properties_enriched = []\n        for column_dict in self.dataset.enriched_column_properties:\n            try:\n                pred = self.enriched_field_json(field_json=column_dict)\n                enriched_fields = json.loads(pred.enriched_field_json)\n            except json.decoder.JSONDecodeError:\n                print(\"Error in decoding JSON for column: \", column_dict['column_name'])\n                continue\n        \n            column_dict['properties'] = {**column_dict['properties'], **enriched_fields}\n        \n            column_properties_enriched.append(column_dict)\n                \n        self.dataset.enriched_column_properties = column_properties_enriched\n        \n        # pprint(self.dataset.column_properties)\n        \n    def enrich_dataset_description(self):\n        \"\"\"\n            Enriches the dataset with description.\n        \"\"\"\n        pred = self.dataset_description(schema=self.dataset.enriched_dataset_schema)\n        self.dataset.enriched_dataset_schema.append({'dataset_description': pred.dataset_description})\n        self.dataset.enriched_column_properties.append({'dataset_description': pred.dataset_description})\n    \n        \n    def forward(self) -> dict:\n        self.enrich_fields()\n        self.enrich_dataset_description()\n        \n        # Return the enriched dataset information\n        print(\"forwarded properties: \", self.dataset.enriched_column_properties)\n        return {\n            'enriched_column_properties': self.dataset.enriched_column_properties,\n            'enriched_dataset_schema': self.dataset.enriched_dataset_schema\n        }", "output": "<module 'dspy' from '/Users/shikharsakhuja/miniforge3/envs/venv_kernel_1/lib/python3.9/site-packages/dspy/__init__.py'>\n# Execution finished\n", "executionCount": 33}, {"id": "878be0c5-e315-4bd0-a176-6300bcd6318c", "code": "class EntrypointParams(BaseModel):\n    csv_file_uri: str\n\ndef entrypoint(data: EntrypointParams):\n    csv_file_uri = data.csv_file_uri\n    enrich = DatasetEnrich(csv_file_uri).forward()\n    return enrich\n\n", "output": "# Execution finished\n", "executionCount": 36}, {"id": "f8dbb3ee-6efc-4219-b313-7c790127fa78", "code": "print(entrypoint(EntrypointParams(**{\"csv_file_uri\": \"s3://llm-data-viz-agentkali/data_uploads/62083a15-665e-4f82-922c-f7f9b63323bb.csv\"})))", "output": "df                               Name        Type  AWD  RWD  Retail_Price  \\\n0                 Acura 3.5 RL 4dr       Sedan    0    0         43755   \n1    Acura 3.5 RL w/Navigation 4dr       Sedan    0    0         46100   \n2                        Acura MDX         SUV    1    0         36945   \n3     Acura NSX coupe 2dr manual S  Sports Car    0    1         89765   \n4             Acura RSX Type S 2dr       Sedan    0    0         23820   \n..                             ...         ...  ...  ...           ...   \n382              Volvo S80 2.9 4dr       Sedan    0    0         37730   \n383               Volvo S80 T6 4dr       Sedan    0    0         45210   \n384                      Volvo V40       Wagon    0    0         26135   \n385                     Volvo XC70       Wagon    1    0         35145   \n386                  Volvo XC90 T6         SUV    1    0         41250   \n\n     Dealer_Cost  Engine_Size__l_  Cyl  Horsepower_HP_  City_Miles_Per_Gallon  \\\n0          39014              3.5    6             225                     18   \n1          41100              3.5    6             225                     18   \n2          33337              3.5    6             265                     17   \n3          79978              3.2    6             290                     17   \n4          21761              2.0    4             200                     24   \n..           ...              ...  ...             ...                    ...   \n382        35542              2.9    6             208                     20   \n383        42573              2.9    6             268                     19   \n384        24641              1.9    4             170                     22   \n385        33112              2.5    5             208                     20   \n386        38851              2.9    6             268                     15   \n\n     Highway_Miles_Per_Gallon  Weight  Wheel_Base  Len  Width  \n0                          24    3880         115  197     72  \n1                          24    3893         115  197     72  \n2                          23    4451         106  189     77  \n3                          24    3153         100  174     71  \n4                          31    2778         101  172     68  \n..                        ...     ...         ...  ...    ...  \n382                        28    3576         110  190     72  \n383                        26    3653         110  190     72  \n384                        29    2822         101  180     68  \n385                        27    3823         109  186     73  \n386                        20    4638         113  189     75  \n\n[387 rows x 15 columns]\nSetting new properties list\n[{'column_name': 'Name',\n  'properties': {'description': 'Names of various car models.',\n                 'dtype': 'string',\n                 'num_unique_values': 385,\n                 'samples': ['Nissan Altima S 4dr',\n                             'Mercury Marauder 4dr',\n                             'Toyota Prius 4dr (gas/electric)'],\n                 'semantic_type': 'car_model_name'}},\n {'column_name': 'Type',\n  'properties': {'description': 'Categorization of different types of '\n                                'vehicles.',\n                 'dtype': 'category',\n                 'num_unique_values': 5,\n                 'samples': ['SUV', 'Minivan', 'Sports Car'],\n                 'semantic_type': 'vehicle_type'}},\n {'column_name': 'AWD',\n  'properties': {'description': 'Indicates the presence or absence of '\n                                'all-wheel drive.',\n                 'dtype': 'number',\n                 'max': 1,\n                 'min': 0,\n                 'num_unique_values': 2,\n                 'samples': [1, 0],\n                 'semantic_type': 'binary_indicator',\n                 'std': 0}},\n {'column_name': 'RWD',\n  'properties': {'description': 'A binary indicator representing a presence or '\n                                'absence state.',\n                 'dtype': 'number',\n                 'max': 1,\n                 'min': 0,\n                 'num_unique_values': 2,\n                 'samples': [1, 0],\n                 'semantic_type': 'binary_indicator',\n                 'std': 0}},\n {'column_name': 'Retail_Price',\n  'properties': {'description': 'The retail price of goods sold.',\n                 'dtype': 'number',\n                 'max': 192465,\n                 'min': 10280,\n                 'num_unique_values': 370,\n                 'samples': [22775, 37245],\n                 'semantic_type': 'price_numeric',\n                 'std': 19724}},\n {'column_name': 'Dealer_Cost',\n  'properties': {'description': 'Represents the cost incurred by a dealer for '\n                                'a product.',\n                 'dtype': 'number',\n                 'max': 173560,\n                 'min': 9875,\n                 'num_unique_values': 384,\n                 'samples': [18030, 31558],\n                 'semantic_type': 'financial_cost',\n                 'std': 17901}},\n {'column_name': 'Engine_Size__l_',\n  'properties': {'description': 'The size of the engine measured in liters.',\n                 'dtype': 'number',\n                 'max': 6.0,\n                 'min': 0.0,\n                 'num_unique_values': 40,\n                 'samples': [2.2, 5.3],\n                 'semantic_type': 'engine_capacity_liter',\n                 'std': 1.0266787710109588}},\n {'column_name': 'Cyl',\n  'properties': {'description': 'Number of cylinders in an engine.',\n                 'dtype': 'number',\n                 'max': 12,\n                 'min': 0,\n                 'num_unique_values': 8,\n                 'samples': [4, 9],\n                 'semantic_type': 'engine_cylinder_count',\n                 'std': 1}},\n {'column_name': 'Horsepower_HP_',\n  'properties': {'description': \"Numerical representation of an engine's \"\n                                'horsepower.',\n                 'dtype': 'number',\n                 'max': 493,\n                 'min': 73,\n                 'num_unique_values': 100,\n                 'samples': [126, 138],\n                 'semantic_type': 'engine_power',\n                 'std': 70}},\n {'column_name': 'City_Miles_Per_Gallon',\n  'properties': {'description': 'Represents the miles per gallon a vehicle '\n                                'achieves in city driving conditions.',\n                 'dtype': 'number',\n                 'max': 1000,\n                 'min': 10,\n                 'num_unique_values': 29,\n                 'samples': [59, 32],\n                 'semantic_type': 'fuel_efficiency',\n                 'std': 50}},\n {'column_name': 'Highway_Miles_Per_Gallon',\n  'properties': {'description': 'A numerical measure of vehicle fuel '\n                                'efficiency on highways in miles per gallon.',\n                 'dtype': 'number',\n                 'max': 66,\n                 'min': -1100,\n                 'num_unique_values': 33,\n                 'samples': [43, 37],\n                 'semantic_type': 'fuel_efficiency',\n                 'std': 57}},\n {'column_name': 'Weight',\n  'properties': {'description': 'A numerical representation of mass for '\n                                'various entities.',\n                 'dtype': 'number',\n                 'max': 6400,\n                 'min': 1850,\n                 'num_unique_values': 315,\n                 'samples': [4473, 3472],\n                 'semantic_type': 'numerical_measurement',\n                 'std': 706}},\n {'column_name': 'Wheel_Base',\n  'properties': {'description': 'Measures the distance between the front and '\n                                'rear axles of a vehicle.',\n                 'dtype': 'number',\n                 'max': 130,\n                 'min': 0,\n                 'num_unique_values': 34,\n                 'samples': [110, 0],\n                 'semantic_type': 'numerical_measurement',\n                 'std': 8}},\n {'column_name': 'Len',\n  'properties': {'description': 'A numerical representation of length values.',\n                 'dtype': 'number',\n                 'max': 221,\n                 'min': 143,\n                 'num_unique_values': 60,\n                 'samples': [197, 183],\n                 'semantic_type': 'numerical_measurement',\n                 'std': 13}},\n {'column_name': 'Width',\n  'properties': {'description': 'A numerical representation of width '\n                                'dimensions.',\n                 'dtype': 'number',\n                 'max': 81,\n                 'min': 2,\n                 'num_unique_values': 19,\n                 'samples': [72, 70],\n                 'semantic_type': 'numerical_measurement',\n                 'std': 4}}]\nNone\n/Users/shikharsakhuja/miniforge3/envs/venv_kernel_1/lib/python3.9/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"schema\" in \"StringSignature\" shadows an attribute in parent \"Signature\"\n  warnings.warn(\n# Execution finished\n", "executionCount": 39}]